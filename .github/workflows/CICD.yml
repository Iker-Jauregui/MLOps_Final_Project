name: CICD
on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]
  workflow_dispatch:

jobs:

  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
    
      - name: Set up uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"
    
      - name: install packages
        run: make install
    
      # Add DVC setup and pull model
      - name: Set up DVC
        uses: iterative/setup-dvc@v1
    
      - name: Configure DVC remote with DagsHub
        env:
          DAGSHUB_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          dvc remote modify storage --local auth basic
          dvc remote modify storage --local user "$DAGSHUB_USERNAME"
          dvc remote modify storage --local password "$DAGSHUB_TOKEN"
    
      - name: Pull production artifacts from DVC
        run: |
          if [ -f production/model/best_rf.onnx.dvc ]; then
            echo "Pulling production model from DVC..."
            dvc pull production/model/best_rf.onnx.dvc
          else
            echo "No production model found"
          fi
          
          if [ -f production/model/categorical_metadata.json.dvc ]; then
            echo "Pulling categorical metadata from DVC..."
            dvc pull production/model/categorical_metadata.json.dvc
          else
            echo "No categorical metadata found"
          fi
    
      - name: format
        run: make format
    
      - name: lint
        run: make lint
    
      - name: test
        run: make test

  # Check if we should train based on commit message
  check-train-flag:
    runs-on: ubuntu-latest
    outputs:
      should_train: ${{ steps.check.outputs.should_train }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Check for TRAIN tag
        id: check
        run: |
          COMMIT_MESSAGE=$(git log -1 --pretty=%B)
          if echo "$COMMIT_MESSAGE" | grep -q "#TRAIN"; then
            echo "should_train=true" >> $GITHUB_OUTPUT
            echo "#TRAIN tag found - will train model"
          else
            echo "should_train=false" >> $GITHUB_OUTPUT
            echo "No #TRAIN tag - skipping training"
          fi

  train-and-serialize:
    runs-on: ubuntu-latest
    needs: [build, check-train-flag]
    if: needs.check-train-flag.outputs.should_train == 'true'
    permissions:
      contents: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"
      
      - name: Install dependencies
        run: make install
      
      - name: Set up DVC
        uses: iterative/setup-dvc@v1
      
      - name: Configure DVC remote with DagsHub
        env:
          DAGSHUB_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          dvc remote modify storage --local auth basic
          dvc remote modify storage --local user "$DAGSHUB_USERNAME"
          dvc remote modify storage --local password "$DAGSHUB_TOKEN"
      
      - name: Pull data from DVC
        run: dvc pull
      
      - name: Verify data exists
        run: |
          echo "Checking for training data..."
          ls -la data/processed/iter1/train/ || echo "Train data directory not found"
          find data/ -name "*.parquet" -type f | head -5 || echo "No parquet files found"
      
      - name: Train model with Optuna and MLflow
        env:
          DAGSHUB_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: uv run --group train scripts/train.py
      
      - name: Serialize best model
        env:
          DAGSHUB_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: uv run --group train scripts/serialize.py
      
      - name: Copy artifacts to production directory
        run: |
          mkdir -p production/model
          
          # Copy ONNX model
          cp models/onnx/best_rf.onnx production/model/best_rf.onnx
          echo "Model copied to production/model/best_rf.onnx"
          
          # Copy categorical metadata JSON
          if [ -f models/onnx/categorical_metadata.json ]; then
            cp models/onnx/categorical_metadata.json production/model/categorical_metadata.json
            echo "Metadata copied to production/model/categorical_metadata.json"
          else
            echo "categorical_metadata.json not found in models/onnx/"
            exit 1
          fi
      
      - name: Add artifacts to DVC
        run: |
          # Add model to DVC
          dvc add production/model/best_rf.onnx
          echo "Model added to DVC"
          
          # Add metadata to DVC
          dvc add production/model/categorical_metadata.json
          echo "Metadata added to DVC"
      
      - name: Commit and push artifacts to Git
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          git add production/model/best_rf.onnx.dvc \
                  production/model/categorical_metadata.json.dvc \
                  production/model/.gitignore
          
          git commit -m "Update production model and metadata [skip ci]" || echo "No changes to commit"
          git push
      
      - name: Push artifacts to DVC remote
        run: |
          dvc push --jobs 1 -v
          echo "Model and metadata pushed to DVC remote (DagsHub)"
      
      - name: Upload model artifacts
        uses: actions/upload-artifact@v4
        with:
          name: trained-model
          path: |
            production/model/best_rf.onnx
            production/model/categorical_metadata.json
          retention-days: 30
      
      - name: Clean up MLflow directory
        if: always()
        run: rm -rf ${{ env.MLFLOW_DIR }}

  deploy:
    runs-on: ubuntu-latest
    needs: [train-and-serialize, check-train-flag]
    if: always() && needs.check-train-flag.result == 'success'
    env:
      DOCKER_IMAGE: ${{ secrets.DOCKERHUB_USERNAME }}/${{ secrets.DOCKERHUB_PROJECT_IMAGE }}
      DOCKER_IMAGE_PROMETHEUS: ${{ secrets.DOCKERHUB_USERNAME }}/${{ secrets.DOCKERHUB_PROM_IMAGE }}
    steps:
      - name: Checkout repository (with submodules)
        uses: actions/checkout@v4 
        with:
          submodules: recursive
          ref: main  # Get the latest code including model commit
      
      - name: Get production artifacts
        env:
          DAGSHUB_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          if [ "${{ needs.check-train-flag.outputs.should_train }}" == "true" ]; then
            echo "Downloading freshly trained artifacts from artifact..."
            mkdir -p production/model
          else
            echo "Pulling existing artifacts from DVC..."
            # Set up DVC
            pip install dvc dvc-gs
            
            # Configure DVC credentials
            dvc remote modify storage --local auth basic
            dvc remote modify storage --local user "$DAGSHUB_USERNAME"
            dvc remote modify storage --local password "$DAGSHUB_TOKEN"
            
            # Pull the production artifacts
            dvc pull production/model/best_rf.onnx.dvc
            dvc pull production/model/categorical_metadata.json.dvc
          fi
      
      - name: Download artifacts (if trained)
        if: needs.check-train-flag.outputs.should_train == 'true'
        uses: actions/download-artifact@v4
        with:
          name: trained-model
          path: production/model/
      
      - name: Verify artifacts exist
        run: |
          # Check model
          if [ ! -f production/model/best_rf.onnx ]; then
            echo "Model not found at production/model/best_rf.onnx"
            exit 1
          fi
          echo "Model ready: $(ls -lh production/model/best_rf.onnx)"
          
          # Check metadata
          if [ ! -f production/model/categorical_metadata.json ]; then
            echo "Metadata not found at production/model/categorical_metadata.json"
            exit 1
          fi
          echo "Metadata ready: $(ls -lh production/model/categorical_metadata.json)"

      - name: Set up QEMU (for buildx)
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      
      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          push: true
          tags: ${{ env.DOCKER_IMAGE }}
          file: ./Dockerfile
          context: .
      
      - name: Trigger Render Deploy
        run: curl -X POST "https://api.render.com/deploy/srv-d52jd0mmcj7s73bppu2g?key=${{ secrets.RENDER_DEPLOY_HOOK_KEY }}"

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          push: true
          tags: ${{ env.DOCKER_IMAGE_PROMETHEUS }}
          file: ./Dockerfile.prometheus
          context: .
      
      - name: Trigger Render Deploy
        run: curl -X POST "https://api.render.com/deploy/srv-d5d5uf5actks73chk0v0?key=${{ secrets.RENDER_PROM_DEPLOY_HOOK_KEY }}"

  deploy-hf:
    runs-on: ubuntu-latest
    needs: deploy
    if: always() && needs.deploy.result == 'success'
    steps:
      - name: Checkout repository with all branches
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: main
      
      - name: Switch to hf-space branch
        run: |
          git fetch origin hf-space
          git checkout hf-space
      
      - name: Add Hugging Face remote
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_USERNAME: ${{ secrets.HF_USERNAME }}
        run: git remote add huggingface https://${HF_USERNAME}:${HF_TOKEN}@huggingface.co/spaces/${HF_USERNAME}/song_revenue_prediction.git

      - name: Push hf-space branch to Hugging Face
        run: git push huggingface hf-space:main --force
